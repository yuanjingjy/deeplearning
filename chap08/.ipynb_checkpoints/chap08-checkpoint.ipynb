{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.6590012 , 0.24243298, 0.09856589], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#导入keras模型，不能使用import keras，它导入的是标准的Keras库\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers#导入常见网络层类\n",
    "#创建Softmax层，并调用__call__方法完成前向计算\n",
    "x = tf.constant([2.,1.,0.1])\n",
    "layer = layers.Softmax(axis=-1)#创建Softmax层\n",
    "layer(x)#调用softmax前向计算\n",
    "#经过softmax网络层后，得到概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.00389306, 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.        ],\n",
       "       [0.        , 0.11099126]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#通过Keras提供的网络容器Sequential将多个网络层封装成一个大网络模型，只需要调用网络模型的实例一次即可完成数据从第一层到最末层的顺序运算\n",
    "#例如，将2层的全连接层加上单独的激活函数层，可以通过Sequential容器封装为一个网络\n",
    "#导入Swquential容器\n",
    "from tensorflow.keras import layers,Sequential\n",
    "network = Sequential([#封装为一个网络\n",
    "    layers.Dense(3,activation=None),#全连接层，3是输出节点个数\n",
    "    layers.ReLU(),#激活函数层\n",
    "    layers.Dense(2,activation=None),#全连接层\n",
    "    layers.ReLU()#激活函数层\n",
    "])\n",
    "x = tf.random.normal([4,3])\n",
    "network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              multiple                  15        \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  12        \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               multiple                  0         \n",
      "=================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "dense_4/kernel:0 (4, 3)\n",
      "dense_4/bias:0 (3,)\n",
      "dense_5/kernel:0 (3, 3)\n",
      "dense_5/bias:0 (3,)\n"
     ]
    }
   ],
   "source": [
    "#Sequential容器也可以通过add()方法继续追加新的网络层，实现动态创建网络的功能\n",
    "layers_num = 2#堆叠次数\n",
    "network = Sequential([])#先创建空的网络\n",
    "for _ in range(layers_num):\n",
    "    network.add(layers.Dense(3))#添加全连接层\n",
    "    network.add(layers.ReLU())#添加激活函数层\n",
    "network.build(input_shape=(None,4))#创建网络参数\n",
    "network.summary()\n",
    "#打印出网络的待优化参数名与shape\n",
    "for p in network.trainable_variables:\n",
    "    print(p.name,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备mnist数据，用于基于Keras的神经网络模型的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n",
      "(128, 784) (128, 10)\n"
     ]
    }
   ],
   "source": [
    "from    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "batchsz = 128\n",
    "(x,y),(x_val,y_val)=datasets.mnist.load_data()\n",
    "print('datasets:',x.shape,y.shape,x.min(),x.max())\n",
    "\n",
    "def preprocess(x,y):\n",
    "      \"\"\"\n",
    "      x is a simple image, not a batch\n",
    "      \"\"\"\n",
    "      x = tf.cast(x,dtype=tf.float32)/255.\n",
    "      x = tf.reshape(x,[28*28])\n",
    "      y = tf.cast(y,dtype=tf.int32)\n",
    "      y = tf.one_hot(y,depth=10)\n",
    "      return x,y\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "db = db.map(preprocess).shuffle(60000).batch(batchsz)\n",
    "ds_val=tf.data.Dataset.from_tensor_slices((x_val,y_val))\n",
    "ds_val = ds_val.map(preprocess).batch(batchsz)\n",
    "\n",
    "sample = next(iter(db))\n",
    "print(sample[0].shape,sample[1].shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在训练网络时，一般的流程是通过前向计算获得网络的输出值，再通过损失函数计算网络误差，然后通过自动求导工具计算梯度并更新，同时间隔性地测试网络的性能。对于这种常用的训练逻辑，可以直接通过Keras提供的模型装配与训练高层接口实现，简洁清晰。\n",
    "#### keras.Model类：网络的母类，除了具有Layer类的功能，还添加了保存、加载模型、训练与测试模型等便捷功能。Sequential也是Model的子类，因此具有Model类的所有功能。\n",
    "#### keras.layers.Layer类，网络层的母类，定义了网络层的一些常见功能，如添加权值，管理权值列表等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             multiple                  330       \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model及其子类的模型装配与训练功能。以Sequential容器封装的网络为例，首先创建5层的\n",
    "#全连接网络用于MNIST手写数字图片识别\n",
    "network = Sequential([layers.Dense(256,activation='relu'),\n",
    "                     layers.Dense(128,activation='relu'),\n",
    "                     layers.Dense(64,activation='relu'),\n",
    "                     layers.Dense(32,activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.build(input_shape=(None,28*28))\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras中提供的compile（）和fit（）函数，方便的实现前向计算与反向传播。compile函数指定网络使用的优化器对象，损失函数，评价指标等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入优化器、损失函数模块\n",
    "from tensorflow.keras import optimizers,losses\n",
    "#采用Adam优化器，学习率为0.01；采用交叉熵损失函数，包含Softmax\n",
    "network.compile(optimizer = optimizers.Adam(lr=0.01),\n",
    "                loss=losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy']#设置测量指标为准确率\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型装配完成后，即可以通过fit()函数送入待训练的数据和验证用的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 469 steps, validate for 79 steps\n",
      "Epoch 1/5\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0785 - accuracy: 0.9789\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0818 - accuracy: 0.9788 - val_loss: 0.1217 - val_accuracy: 0.9729\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0684 - accuracy: 0.9824\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0792 - accuracy: 0.9801 - val_loss: 0.1115 - val_accuracy: 0.9752\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0662 - accuracy: 0.9830\n",
      "79/79 [==============================] - 1s 14ms/step - loss: 0.1201 - accuracy: 0.9761\n",
      "tf.Tensor(\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7\n",
      " 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9\n",
      " 1 7 3 2 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8\n",
      " 7 3 9 7 9 4 4 9 2 5 4 7 6 4 9 0 5], shape=(128,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7\n",
      " 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9\n",
      " 1 7 3 2 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8\n",
      " 7 3 9 7 4 4 4 9 2 5 4 7 6 7 9 0 5], shape=(128,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#指定训练集为train_db,验证集为val_db,训练5个epochs，每2个epoch验证一次\n",
    "#返回训练信息保存在history中\n",
    "history = network.fit(db,epochs=5,validation_data=ds_val,validation_freq=2)\n",
    "history.history\n",
    "network.evaluate(ds_val)\n",
    "sample = next(iter(ds_val))\n",
    "x = sample[0]\n",
    "y = sample[1]#one-hot\n",
    "pred = network.predict(x)#[b,10]\n",
    "#convert back to number\n",
    "y = tf.argmax(y,axis=1)\n",
    "pred = tf.argmax(pred,axis=1)\n",
    "\n",
    "print(pred)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential_9 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-27d5d40f9f93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#方法一：张量方式：保存weights张量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#保存模型参数到文件上\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weights.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saved weights'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;31m#删除网络对象\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0munknown\u001b[0m \u001b[0mformat\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m     \"\"\"\n\u001b[1;32m-> 1069\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m     \u001b[0mfilepath_is_h5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1472\u001b[0m                        \u001b[1;34m'Weights are created when the Model is first called on '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m                        \u001b[1;34m'inputs or `build()` is called with an `input_shape`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                        self.name)\n\u001b[0m\u001b[0;32m   1475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_graph_network_add_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbolic_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Weights for model sequential_9 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "#网络的保存，未防止训练过程中断，定时对网络训练结果进行保存，以便恢复\n",
    "\n",
    "\n",
    "#方法一：张量方式：保存weights张量\n",
    "#保存模型参数到文件上\n",
    "network.save_weights('weights.ckpt')\n",
    "print('saved weights')\n",
    "del network#删除网络对象\n",
    "#重新创建相同的网络结构\n",
    "network = Sequential([layers.Dense(256,activation='relu'),\n",
    "                     layers.Dense(128,activation='relu'),\n",
    "                     layers.Dense(64,activation='relu'),\n",
    "                     layers.Dense(32,activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.compile(optimizer = optimizers.Adam(lr=0.01),\n",
    "                loss=losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy']#设置测量指标为准确率\n",
    "               )\n",
    "#从参数文件中读取数据并写入当前网络\n",
    "network.load_weights('weights.ckpt')\n",
    "print('loaded weights!')\n",
    "\n",
    "\n",
    "#方法二：网络方式：利用Model.save（path）函数将模型的结构以及模型的参数保存到path\n",
    "#文件上，不需要网络源文件的情况下恢复出网络结构和参数\n",
    "#保存模型的结构与模型的参数到文件\n",
    "network.save('model.h5')\n",
    "print('saved total model.')\n",
    "del network#删除网络对象\n",
    "#通过model.h5文件即可恢复出网络的结构和状态\n",
    "network = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "##方法三：SavedModel方式:通过tf.keras.experimental.export_saved_model(network,path)\n",
    "#即可将模型保存再path目录中，方便各个平台（移动端和网页端）能够无缝对接训练好的网络模型\n",
    "tf.keras.experimental.export_saved_model(network,'model-savedmodel')\n",
    "del network\n",
    "#从文件恢复网络结构与网络参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建自定义网络层，需要继承自layers.Layer基类;创建自定义网络类，需要继承自Keras.Model基类，这样产生的基类才能够方便利用Layer/Model基类提供的参数管理功能，同时也能与其他的标准网络层类交互使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Variable 'Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
       "  array([[ 0.5229539 , -0.63890463,  0.37991267],\n",
       "         [-1.1160955 , -0.24662398,  0.903859  ],\n",
       "         [ 0.03559241, -1.2720702 , -0.5865275 ],\n",
       "         [-0.70018685,  0.9511368 ,  0.45431098]], dtype=float32)>],\n",
       " [])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#自定义网络层\n",
    "#对于自定义的网络层，需要实现初始化__init__方法和前向传播逻辑call方法\n",
    "class MyDense(layers.Layer):\n",
    "    #自定义网络层\n",
    "    def __init__(self,inp_dim,outp_dim):\n",
    "        super(MyDense,self).__init__()\n",
    "        #创建权值张量并添加到类管理列表中，设置为需要优化\n",
    "        self.kernel = self.add_weight('w',[inp_dim,outp_dim],trainable=True)\n",
    "        #通过tf.Variable创建的类成员也会自动加入类参数列表\n",
    "        self.kernel = tf.Variable(tf.random.normal([inp_dim,outp_dim]),trainable=False)\n",
    "    #完成自定义类的初始化工作后，设计自定义类的前向运算逻辑\n",
    "    def call(self,inpputs,training=None):#training为True时执行训练模式，False测试模式，莫默认None，测试模式\n",
    "        #实现自定义类的前向计算逻辑\n",
    "        #X@W\n",
    "        out = input@self.kernel\n",
    "        #执行激活函数运算\n",
    "        out = tf.nn.relu(out)\n",
    "        return out\n",
    "    \n",
    "net = MyDense(4,3)#创建输入为4，输出为3节点的自定义层\n",
    "net.variables,net.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_dense_3 (MyDense)         multiple                  200704    \n",
      "_________________________________________________________________\n",
      "my_dense_4 (MyDense)         multiple                  32768     \n",
      "_________________________________________________________________\n",
      "my_dense_5 (MyDense)         multiple                  8192      \n",
      "_________________________________________________________________\n",
      "my_dense_6 (MyDense)         multiple                  2048      \n",
      "_________________________________________________________________\n",
      "my_dense_7 (MyDense)         multiple                  320       \n",
      "=================================================================\n",
      "Total params: 244,032\n",
      "Trainable params: 0\n",
      "Non-trainable params: 244,032\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#自定义网络：基于上述“无偏置的全连接层”实现MNIST手写数字图片模型的创建\n",
    "network = Sequential([MyDense(784,256),#使用自定义的层\n",
    "                     MyDense(256,128),\n",
    "                     MyDense(128,64),\n",
    "                     MyDense(64,32),\n",
    "                     MyDense(32,10)])\n",
    "network.build(input_shape=(None,28*28))\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更普遍地，可以继承基类来实现任意逻辑的自定义网络类。\n",
    "class MyModel(keras.Model):\n",
    "    #自定义网络类，继承自Model基类\n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        #完成网络内需要的网络层的创建工作\n",
    "        self.fc1 = MyDense(28*28,256)\n",
    "        self.fc2 = MyDense(256,128)\n",
    "        self.fc3 = MyDense(128,64)\n",
    "        self.fc4 = MyDense(64,32)\n",
    "        self.fc5 = MyDense(32,10)\n",
    "    #然后实现自定义网络的前运算逻辑,自定义网络的前向逻辑可以任意定制，不像Swquential\n",
    "    #包裹的一次调用每个网络的前向传播函数，各有优缺点。\n",
    "    def call(self,inputs,training=None):\n",
    "        #自定义前向运算逻辑\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root)._training_endpoints\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer-4.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 210s 2us/step\n",
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, None, None, 6 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, None, None, 6 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, None, None, 6 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, None, None, 6 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, None, None, 6 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, None, None, 6 4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, None, None, 6 256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, None, None, 6 0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, None, None, 6 256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, None, None, 6 0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, None, None, 2 16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, None, None, 2 1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, None, None, 2 0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, None, None, 2 0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, None, None, 6 16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, None, None, 6 256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, None, None, 6 0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, None, None, 6 256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, None, None, 6 0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, None, None, 2 0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, None, None, 2 0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, None, None, 6 16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, None, None, 6 256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, None, None, 6 0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, None, None, 6 256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, None, None, 6 0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, None, None, 2 0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, None, None, 2 0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, None, None, 1 32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, None, None, 1 512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, None, None, 1 0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, None, None, 1 512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, None, None, 1 0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, None, None, 5 131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, None, None, 5 2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, None, None, 5 0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, None, None, 5 0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, None, None, 1 512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, None, None, 1 0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, None, None, 1 512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, None, None, 1 0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, None, None, 5 0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, None, None, 5 0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, None, None, 1 512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, None, None, 1 0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, None, None, 1 512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, None, None, 1 0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, None, None, 5 0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, None, None, 5 0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, None, None, 1 512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, None, None, 1 0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, None, None, 1 512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, None, None, 1 0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, None, None, 5 0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, None, None, 5 0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, None, None, 2 131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, None, None, 2 0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, None, None, 2 0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, None, None, 1 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, None, None, 1 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, None, None, 1 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, None, None, 1 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, None, None, 2 0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, None, None, 2 0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, None, None, 1 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, None, None, 1 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, None, None, 2 0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, None, None, 2 0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, None, None, 1 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, None, None, 1 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, None, None, 2 0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, None, None, 2 0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, None, None, 1 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, None, None, 1 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, None, None, 2 0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, None, None, 2 0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, None, None, 1 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, None, None, 1 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, None, None, 2 0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, None, None, 2 0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, None, None, 1 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, None, None, 1 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, None, None, 5 524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, None, None, 5 0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, None, None, 5 0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, None, None, 2 2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, None, None, 2 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, None, None, 2 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, None, None, 2 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, None, None, 5 1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, None, None, 5 0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, None, None, 5 0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, None, None, 2 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, None, None, 2 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, None, None, 5 1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, None, None, 5 0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, None, None, 5 0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, None, None, 2 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, None, None, 2 0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 7, 7, 2048])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kears.application子模块下的常用网络模型；\n",
    "#以ResNet50迁移学习为例，即利用ImageNet上面预训练的特征提取方法迁移到自定义的数据集上，\n",
    "#并根据自定义任务的类别追加一个对应数据类别数的全连接分类层，从而可以再与训练的网络的基础上可以\n",
    "#快速、高效的学习新任务。\n",
    "\n",
    "#首先利用Keras模型乐园加载ImageNet预训练的ResNet50网络，并去掉最后一层\n",
    "resnet = keras.applications.ResNet50(weights = 'imagenet', include_top=False)\n",
    "resnet.summary()\n",
    "#测试网络的输出\n",
    "x = tf.random.normal([4,224,224,3])\n",
    "out = resnet(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2048)\n",
      "(4, 100)\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 100)               204900    \n",
      "=================================================================\n",
      "Total params: 23,792,612\n",
      "Trainable params: 23,739,492\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#对于某个具体任务，需要设置自定义的输出节点数，以100类的分类任务为例，\n",
    "#在ResNet50的基础上重新构建新网络。\n",
    "#新建一个池化层（维度缩减功能），将特征从[b,7,7,2018]降维到[b,2048]\n",
    "#新建池化层\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "#利用上一层的输出作为本层的输入，测试其输出\n",
    "x = tf.random.normal([4,7,7,2048])\n",
    "out = global_average_layer(x)#池化层降维\n",
    "print(out.shape)\n",
    "\n",
    "#最后新建一个全连接层，并设置输出节点数为100\n",
    "fc = layers.Dense(100)\n",
    "#利用上一层的输出作为本层的输入，测试其输出\n",
    "x = tf.random.normal([4,2048])\n",
    "out = fc(x)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "#在得到预训练的ResNet50特征层和我们新建的池化层、全连接层后，重新利用Sequentil容器封装程\n",
    "#一个心电网络\n",
    "mynet = Sequential([resnet,global_average_layer,fc])\n",
    "mynet.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####                               8.6 测试工具\n",
    "#### keras.metrics，专门用于统计训练过程中需要的指标数据\n",
    "#### keras的测量工具的使用一般有4个基本操作流程：新建测量器，写入数据，读取统计数据和清零测量器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#新建测量器：统计平均值的Mean类，统计准确率的Accuracy类，统计余弦相似度的CosineSimilarity类\n",
    "#新建一个平均测量器，适合Loss数据\n",
    "loss_meter = metrics.Mean()\n",
    "\n",
    "#通过测量器的update_state函数可以写入新数据\n",
    "#记录采样的数据\n",
    "loss_meter.update_state(float(loss))#防止在每个batch运算完成后，测量器会自动根据采样的数据来统计平均值\n",
    "\n",
    "#读取统计信息：在采样多次后，可以用测量器的result()函数获取统计值\n",
    "#打印统计的平均loss\n",
    "print(step,'loss:',loss_meter.result())\n",
    "\n",
    "#清除，reset_staets(),每次读取完平均误差后，清零统计信息，以便下一轮统计的开始\n",
    "if step % 100 == 0:\n",
    "    #打印统计的平均loss\n",
    "    print(step,'loss:',loss_meter.result())\n",
    "    loss_meter.rest_states()#清零测量器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             multiple                  330       \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 loss: 2.2945707\n",
      "78 Evaluate Acc: 0.1062 0.1062\n",
      "100 loss: 0.52816534\n"
     ]
    }
   ],
   "source": [
    "#准确率统计实战\n",
    "import  tensorflow as tf\n",
    "from    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "\n",
    "    return x,y\n",
    "\n",
    "\n",
    "batchsz = 128\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
    "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
    "\n",
    "\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "db = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n",
    "\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "ds_val = ds_val.map(preprocess).batch(batchsz) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "network = Sequential([layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(128, activation='relu'),\n",
    "                     layers.Dense(64, activation='relu'),\n",
    "                     layers.Dense(32, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.build(input_shape=(None, 28*28))\n",
    "network.summary()\n",
    "\n",
    "optimizer = optimizers.Adam(lr=0.01)\n",
    "\n",
    "acc_meter = metrics.Accuracy()\n",
    "loss_meter = metrics.Mean()\n",
    "\n",
    "\n",
    "for step, (x,y) in enumerate(db):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # [b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        # [b, 784] => [b, 10]\n",
    "        out = network(x)\n",
    "        # [b] => [b, 10]\n",
    "        y_onehot = tf.one_hot(y, depth=10) \n",
    "        # [b]\n",
    "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n",
    "\n",
    "        loss_meter.update_state(loss)\n",
    "\n",
    " \n",
    "\n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "\n",
    "\n",
    "    if step % 100 == 0:\n",
    "\n",
    "        print(step, 'loss:', loss_meter.result().numpy()) \n",
    "        loss_meter.reset_states()\n",
    "\n",
    "\n",
    "    # evaluate\n",
    "    if step % 500 == 0:\n",
    "        total, total_correct = 0., 0\n",
    "        acc_meter.reset_states()\n",
    "\n",
    "        for step, (x, y) in enumerate(ds_val): \n",
    "            # [b, 28, 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # [b, 784] => [b, 10]\n",
    "            out = network(x) \n",
    "\n",
    "\n",
    "            # [b, 10] => [b] \n",
    "            pred = tf.argmax(out, axis=1) \n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # bool type \n",
    "            correct = tf.equal(pred, y)\n",
    "            # bool tensor => int tensor => numpy\n",
    "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
    "            total += x.shape[0]\n",
    "\n",
    "            acc_meter.update_state(y, pred)\n",
    "\n",
    "\n",
    "        print(step, 'Evaluate Acc:', total_correct/total, acc_meter.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
